# Marketplace insights

University students in general have **hectic schedules**, the stress and anxiety levels they experience are higher compared to the normal human. Likely, if they were to approach my chatbot, they would have had a negative experience on the online marketplace. In order to **accommodate the heightened frustration and stress** of users that use my chatbot, my chatbot would thus have to deliver responses in a tone and style that is **understanding**. The chatbot should also be **performant in the aspect of quality of responses**, any subpar responses could lead to users leaving the marketplace out of anger and frustration. The chatbot thus has to be tested and iterated rigorously so that it can handle various scenarios while properly escalating scenarios that are out of its scope.

Since campus communities are a small population, I would assume the human moderator team would too, be small. Even though the campus community population might be small, the **ratio of moderator to user** remains skewed. Therefore, scenarios that are escalated to human moderators should contain as much useful information as possible to help moderators streamline troubleshooting processes as **efficiently as possible**. This too, is achieved through rigorous testing and iterating of the prompt used for the chatbot.

Like any marketplace, there exist **seasonal trends**. The number of users will peak at certain periods of time. For a university marketplace, peak user periods would include end of semesters, this is because this is the time when students graduate, students move out of on campus accommodation or students no longer require textbooks and notes they have bought or written for the semester. Our chatbot would thus have to scale according to user count peaks and dips. This would likely involve our chatbot being containerised and deployed onto a cloud service so that our chatbot can **autoscale** with its number of users.

Though the chatbot helps alleviate the workload of a human moderator team on the online university marketplace, it should not be alleviating all of it. Leaving an AI chatbot to handle community safety and trust is not a good idea in the slightest. There should be a **balance between automation and maintaining customer trust**. The chatbot should be tasked to handle most trivial tasks, leaving complex tasks to the human moderators. These complex tasks include problems that surface **offline**, such as scams, abuse and stealing. The references provided to the chatbot thus have to be **well defined and detailed** so that it only handles tasks it's supposed to, leaving more complex tasks that build customer trust to the humans.
